{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f29c6b19-e770-4930-a1fe-766cdcf01b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "60b6ff7f-faaf-47d7-b86b-c9cc73225524",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time_data.pickle', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "contract_lookup = dict([(contract['id'], contract) for contract, series in raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f5e1448d-e9ba-4482-92e3-632658821579",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict([(contract['id'],float(contract['liquidity'])) for contract in contract_lookup.values() if float(contract['liquidity'])>= 100000])\n",
    "interested_ids = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "def in_range(v,l,h):\n",
    "    if v<h and v>l:\n",
    "        return True\n",
    "    return False\n",
    "y_ids = [key for key in interested_ids.keys() if in_range([float(value) for value in ast.literal_eval(contract_lookup[key]['outcomePrices'])][0], .1,.9)][0:5]\n",
    "x_ids = [id for id in interested_ids.keys() if id not in y_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e754713-bcc6-4289-a19f-f2f598f3ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(contract['id'], series['t'], series['p']) for contract, series in raw if contract['id'] in interested_ids.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d699d94-d29e-4d32-8458-7fa4f67bede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Collect all the data into a list of dictionaries\n",
    "rows = []\n",
    "for row in data:\n",
    "    col_name = row[0]\n",
    "    indices = row[1]\n",
    "    values = row[2]\n",
    "    for idx, val in zip(indices, values):\n",
    "        rows.append({'index': idx, 'column': col_name, 'value': val})\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pl.DataFrame(rows)\n",
    "\n",
    "# Pivot the DataFrame to get the desired format\n",
    "df = df.pivot(\n",
    "    values='value',\n",
    "    index='index',\n",
    "    on='column',\n",
    "    aggregate_function='first'  # or 'sum', 'mean', etc., depending on your needs\n",
    ")\n",
    "df = df.fill_null(strategy=\"forward\")\n",
    "\n",
    "# display(df,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "729ec5bf-179c-4229-a8a0-77ac6d791fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X, y, w):\n",
    "  return np.mean(np.log(1 + np.exp(-y * np.dot(X, w))))\n",
    "\n",
    "\n",
    "def logistic_reg(X, y, w_init, max_its, eta, grad_threshold, lam, reg):\n",
    "  assert reg in [1,2]\n",
    "  iteration = 0\n",
    "  w = w_init\n",
    "  grads = []\n",
    "  while iteration < max_its:\n",
    "    denom = 1 + np.exp(y * np.dot(X, w))\n",
    "    temp = (y / denom)\n",
    "    temp = (y / denom)[:, np.newaxis] * X\n",
    "    grad = -np.sum(temp, axis=0) / len(X)\n",
    "    v = grad\n",
    "    iteration += 1\n",
    "    grads.append(v)\n",
    "    abs_grad = np.abs(grad)\n",
    "    if reg == 1:\n",
    "      w_new = w - eta * v\n",
    "      w = np.sign(w_new) * np.maximum(0, np.abs(w_new) - eta * lam)\n",
    "\n",
    "    elif reg == 2:\n",
    "      w =(1-2 *eta * lam) * w - eta * v\n",
    "    if max(abs_grad) < grad_threshold:\n",
    "      break\n",
    "\n",
    "  return w, iteration,cross_entropy(X,y,w),grads\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def data_splitting(X_in,y_col,data):\n",
    "    X = data.select(X_in).to_numpy()\n",
    "    y = data.select([y_col]).to_numpy()\n",
    "    y = y[~np.isnan(y)]\n",
    "    X = X[len(X) - len(y):]\n",
    "    y = y.reshape(len(X))\n",
    "    return X,y\n",
    "\n",
    "def logistic_alpha_models(x_cols, y_cols, data):\n",
    "    dummy = data.drop('index')\n",
    "    weight_columns = dummy.columns\n",
    "    weights = []\n",
    "    test_error = []\n",
    "    for col in y_cols:\n",
    "        X,y = data_splitting(x_cols, col , dummy)\n",
    "        x_train, x_test = train_test_split(X)\n",
    "        y_train, y_test = train_test_split(y)\n",
    "        w, iteration,error,grads = logistic_reg(x_train, y_train, np.zeros(len(train_x[0])), 10**4, .1, 10**-4, .05, 1)\n",
    "        Eout = cross_entropy(x_test, y_test,w)\n",
    "        weights.append(w)\n",
    "        test_error.append(Eout)\n",
    "    return weight_columns, weights, test_error\n",
    "model1 = logistic_alpha_models(x_ids, y_ids, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8efab27d-4b3e-4bee-a729-173e0d1b726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toRNNdata(x,y):\n",
    "    rx_train =[]\n",
    "    ry_train = []\n",
    "    steps = 20\n",
    "    for i in range(steps, x.shape[0]-steps):\n",
    "        rx_train.append(x[i-steps:i, :])\n",
    "        ry_train.append(y[i])\n",
    "    rx_train, ry_train = np.array(rx_train), np.array(ry_train)\n",
    "    return rx_train,ry_train\n",
    "def LSTMGRU(x_train,y_train):\n",
    "    rx_train,ry_train = toRNNdata(x_train,y_train)\n",
    "\n",
    "    epochs = 30\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences = True, input_shape = (rx_train.shape[1],rx_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.fit(rx_train, y_train[40:], batch_size = 32, epochs = epochs,verbose=False)\n",
    "    return model\n",
    "\n",
    "def RNN_alpha_models(x_cols,y_cols,data):\n",
    "    dummy = data.drop('index')\n",
    "    weight_columns = dummy.columns\n",
    "    weights = []\n",
    "    test_error = []\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    rnns = []\n",
    "    results = []\n",
    "    for col in y_cols:\n",
    "        X,y = data_splitting(x_cols, col , dummy)\n",
    "        x_train, x_test = train_test_split(X)\n",
    "        y_train, y_test = train_test_split(y)\n",
    "        rnn = LSTMGRU(x_train,y_train)\n",
    "        rx_test,ry_test = toRNNdata(x_test,y_test)\n",
    "        results = rnn.evaluate(rx_test, ry_test, batch_size=32, verbose = False)\n",
    "        rnns.append((rnn,results))\n",
    "    return rnns\n",
    "model1 = RNN_alpha_models(x_ids, y_ids, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9482779b-5461-4dcb-9ef2-fff90535b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(x,y):\n",
    "    clf = linear_model.LinearRegression()\n",
    "    clf.fit(x,y)\n",
    "    return clf\n",
    "\n",
    "def Linear_alpha_models(x_cols,y_cols,data):\n",
    "    dummy = data.drop('index')\n",
    "    weight_columns = dummy.columns\n",
    "    weights = []\n",
    "    test_error = []\n",
    "    for col in y_cols:\n",
    "        X,y = data_splitting(x_cols, col , dummy)\n",
    "        x_train, x_test = train_test_split(X)\n",
    "        y_train, y_test = train_test_split(y)\n",
    "        lin_model = Linear(x_train,y_train)\n",
    "        yhat = lin_model.predict(x_test)\n",
    "        weights.append(lin_model)\n",
    "        test_error.append((np.square(np.subtract(y_test,yhat)).mean()))\n",
    "    return weight_columns, weights, test_error\n",
    "model2 = Linear_alpha_models(x_ids, y_ids, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "24ea534e-dc97-4f88-89f7-f9d51b6f2ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.09095723,  0.35192552, -0.49715675, -0.39085878,  0.17469247,\n",
       "        0.72878678, -1.83564209, -0.12488592,  0.40590425, -0.04904839,\n",
       "       -0.0559849 , -0.2919703 , -1.05754497, -0.62091484,  0.72814755,\n",
       "        0.33833176, -0.51591335, -0.21025536,  0.49567783,  0.51382667,\n",
       "       -0.11969304, -0.08539693,  0.12879632, -0.00930146,  0.09403661,\n",
       "        0.03959986, -1.07982265,  0.74793248, -1.29828806,  0.48013153,\n",
       "        0.37360764,  0.7491991 , -0.15392427,  0.27660875,  0.02005387,\n",
       "       -0.12283886,  0.06068352,  0.11808976,  0.03536876,  0.00989839,\n",
       "       -0.19870934,  0.06288139,  0.06107794,  0.1024381 , -0.10563867,\n",
       "       -0.11041665, -0.04150815])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2[1][3].coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
